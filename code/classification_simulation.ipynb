{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Simulated Classifcation\n",
    "1. State assumptions\n",
    "2. Formally define classification/regression problem\n",
    "3. provide algorithm for solving problem (including choosing hyperparameters as appropriate)\n",
    "4. sample data from a simulation setting inspired by your data (from both null and alternative as defined before)\n",
    "5. compute accuracy\n",
    "6. plot accuracy vs. sample size in simulation\n",
    "7. apply method directly on real data\n",
    "8. explain the degree to which you believe the result and why\n",
    " \n",
    "### Step 1: State assumptions\n",
    "$F_{X|0} = ER(p_0) = Bern(p_0)^{V \\times V}$ <br/>\n",
    "$F_{X|1} = ER(p_1) = Bern(p_1)^{V \\times V}$\n",
    "\n",
    "$p_1 \\neq p_2$\n",
    "\n",
    "### Step 2: Formally define classification/regression problem\n",
    "$G_i, Y_i \\sim \\mathscr{F}_{G,Y} = \\{ F_{G,Y}(\\cdot; \\theta) : \\theta \\in \\Theta \\}$.\n",
    "\n",
    "Since, all samples observed are graph matched (i.e. nodes are equal across graphs), we can look at just the distribution of adjacency matrices:\n",
    "\n",
    "$F_{G,Y} = F_{X,Y}$.\n",
    "\n",
    "Thus,\n",
    "\n",
    "$X_i = \\prod_{u,v}^{\\mathcal{E}} A_{uv}$, where $\\mathcal{E} \\subset V \\times V$ <br/>\n",
    "$Y_i = \\{0,1\\}$\n",
    "\n",
    "As we are doing classification, we are trying to minimize expected error. Here, expected error can be defined as:\n",
    "\n",
    "$E[l] = \\sum \\Theta(\\hat{Y}_i \\neq Y_i)$\n",
    "\n",
    "Where $\\Theta$ is the indicator function.\n",
    "\n",
    "### Step 3: Provide algorithm for solving problem (including choosing hyperparameters as appropriate)\n",
    "classification:\n",
    "- lda (linear discriminant analysis): no parameters\n",
    "- qda (quadratic discriminant analysis): no parameters\n",
    "- svm (support vector machine): penalty parameters set to 0.5 because it was a default suggested \n",
    "- knn (k-nearest neighbours): number of neighbors set to 3 because it was a default suggested\n",
    "- rf (random forest): like the above, I didn't have better insight so went with defaults. Seemed like a simple starting point, as we always aim for.\n",
    "\n",
    "regression: linear regression, support vector regression, k-nearest neighbour regression, random forest regression, polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import igraph as ig\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12345678)  # for reproducibility, set random seed\n",
    "r = 20  # define number of rois\n",
    "N = 100 # number of samples at each iteration\n",
    "p0 = 0.10\n",
    "p1 = 0.15\n",
    "# define number of subjects per class\n",
    "S = np.array((8, 16, 20, 32, 40, 64, 80, 100, 120, 200, 320,\n",
    "              400, 800, 1000))\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Random Forest\",\n",
    "         \"Linear Discriminant Analysis\", \"Quadratic Discriminant Analysis\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Steps 4 & 5:  Sample data from setting similar to data and record classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Nearest Neighbors: 1.00 (+/- 0.00)\n",
      "Accuracy of Linear SVM: 1.00 (+/- 0.00)\n",
      "Accuracy of Random Forest: 1.00 (+/- 0.00)\n",
      "Accuracy of Linear Discriminant Analysis: 1.00 (+/- 0.00)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.50 (+/- 1.00)\n",
      "Accuracy of Nearest Neighbors: 0.88 (+/- 0.66)\n",
      "Accuracy of Linear SVM: 0.94 (+/- 0.48)\n",
      "Accuracy of Random Forest: 0.94 (+/- 0.48)\n",
      "Accuracy of Linear Discriminant Analysis: 0.94 (+/- 0.48)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.88 (+/- 0.66)\n",
      "Accuracy of Nearest Neighbors: 1.00 (+/- 0.00)\n",
      "Accuracy of Linear SVM: 1.00 (+/- 0.00)\n",
      "Accuracy of Random Forest: 0.95 (+/- 0.44)\n",
      "Accuracy of Linear Discriminant Analysis: 1.00 (+/- 0.00)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.95 (+/- 0.44)\n",
      "Accuracy of Nearest Neighbors: 0.97 (+/- 0.35)\n",
      "Accuracy of Linear SVM: 0.97 (+/- 0.35)\n",
      "Accuracy of Random Forest: 0.97 (+/- 0.35)\n",
      "Accuracy of Linear Discriminant Analysis: 0.97 (+/- 0.35)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.97 (+/- 0.35)\n",
      "Accuracy of Nearest Neighbors: 0.78 (+/- 0.84)\n",
      "Accuracy of Linear SVM: 0.90 (+/- 0.60)\n",
      "Accuracy of Random Forest: 0.90 (+/- 0.60)\n",
      "Accuracy of Linear Discriminant Analysis: 0.90 (+/- 0.60)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.88 (+/- 0.66)\n",
      "Accuracy of Nearest Neighbors: 0.88 (+/- 0.66)\n",
      "Accuracy of Linear SVM: 0.88 (+/- 0.66)\n",
      "Accuracy of Random Forest: 0.83 (+/- 0.75)\n",
      "Accuracy of Linear Discriminant Analysis: 0.91 (+/- 0.58)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.84 (+/- 0.73)\n",
      "Accuracy of Nearest Neighbors: 0.95 (+/- 0.44)\n",
      "Accuracy of Linear SVM: 0.96 (+/- 0.38)\n",
      "Accuracy of Random Forest: 0.95 (+/- 0.44)\n",
      "Accuracy of Linear Discriminant Analysis: 0.96 (+/- 0.38)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.97 (+/- 0.31)\n",
      "Accuracy of Nearest Neighbors: 0.90 (+/- 0.60)\n",
      "Accuracy of Linear SVM: 0.95 (+/- 0.44)\n",
      "Accuracy of Random Forest: 0.90 (+/- 0.60)\n",
      "Accuracy of Linear Discriminant Analysis: 0.95 (+/- 0.44)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.94 (+/- 0.47)\n",
      "Accuracy of Nearest Neighbors: 0.93 (+/- 0.53)\n",
      "Accuracy of Linear SVM: 0.96 (+/- 0.40)\n",
      "Accuracy of Random Forest: 0.93 (+/- 0.50)\n",
      "Accuracy of Linear Discriminant Analysis: 0.95 (+/- 0.44)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.97 (+/- 0.36)\n",
      "Accuracy of Nearest Neighbors: 0.96 (+/- 0.37)\n",
      "Accuracy of Linear SVM: 0.95 (+/- 0.41)\n",
      "Accuracy of Random Forest: 0.96 (+/- 0.39)\n",
      "Accuracy of Linear Discriminant Analysis: 0.94 (+/- 0.46)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.95 (+/- 0.44)\n",
      "Accuracy of Nearest Neighbors: 0.92 (+/- 0.54)\n",
      "Accuracy of Linear SVM: 0.93 (+/- 0.51)\n",
      "Accuracy of Random Forest: 0.91 (+/- 0.57)\n",
      "Accuracy of Linear Discriminant Analysis: 0.93 (+/- 0.51)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.93 (+/- 0.52)\n",
      "Accuracy of Nearest Neighbors: 0.92 (+/- 0.54)\n",
      "Accuracy of Linear SVM: 0.94 (+/- 0.49)\n",
      "Accuracy of Random Forest: 0.92 (+/- 0.54)\n",
      "Accuracy of Linear Discriminant Analysis: 0.94 (+/- 0.48)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.92 (+/- 0.55)\n",
      "Accuracy of Nearest Neighbors: 0.92 (+/- 0.53)\n",
      "Accuracy of Linear SVM: 0.93 (+/- 0.50)\n",
      "Accuracy of Random Forest: 0.94 (+/- 0.47)\n",
      "Accuracy of Linear Discriminant Analysis: 0.95 (+/- 0.45)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.94 (+/- 0.48)\n",
      "Accuracy of Nearest Neighbors: 0.90 (+/- 0.59)\n",
      "Accuracy of Linear SVM: 0.94 (+/- 0.49)\n",
      "Accuracy of Random Forest: 0.94 (+/- 0.49)\n",
      "Accuracy of Linear Discriminant Analysis: 0.94 (+/- 0.48)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.93 (+/- 0.51)\n",
      "[[[ 1.          0.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 0.5         0.5       ]]\n",
      "\n",
      " [[ 0.875       0.33071891]\n",
      "  [ 0.9375      0.24206146]\n",
      "  [ 0.9375      0.24206146]\n",
      "  [ 0.9375      0.24206146]\n",
      "  [ 0.875       0.33071891]]\n",
      "\n",
      " [[ 1.          0.        ]\n",
      "  [ 1.          0.        ]\n",
      "  [ 0.95        0.21794495]\n",
      "  [ 1.          0.        ]\n",
      "  [ 0.95        0.21794495]]\n",
      "\n",
      " [[ 0.96875     0.17399264]\n",
      "  [ 0.96875     0.17399264]\n",
      "  [ 0.96875     0.17399264]\n",
      "  [ 0.96875     0.17399264]\n",
      "  [ 0.96875     0.17399264]]\n",
      "\n",
      " [[ 0.775       0.41758233]\n",
      "  [ 0.9         0.3       ]\n",
      "  [ 0.9         0.3       ]\n",
      "  [ 0.9         0.3       ]\n",
      "  [ 0.875       0.33071891]]\n",
      "\n",
      " [[ 0.875       0.33071891]\n",
      "  [ 0.875       0.33071891]\n",
      "  [ 0.828125    0.37727176]\n",
      "  [ 0.90625     0.2914806 ]\n",
      "  [ 0.84375     0.36309219]]\n",
      "\n",
      " [[ 0.95        0.21794495]\n",
      "  [ 0.9625      0.18998355]\n",
      "  [ 0.95        0.21794495]\n",
      "  [ 0.9625      0.18998355]\n",
      "  [ 0.975       0.15612495]]\n",
      "\n",
      " [[ 0.9         0.3       ]\n",
      "  [ 0.95        0.21794495]\n",
      "  [ 0.9         0.3       ]\n",
      "  [ 0.95        0.21794495]\n",
      "  [ 0.94        0.23748684]]\n",
      "\n",
      " [[ 0.925       0.26339134]\n",
      "  [ 0.95833333  0.19982631]\n",
      "  [ 0.93333333  0.24944383]\n",
      "  [ 0.95        0.21794495]\n",
      "  [ 0.96666667  0.17950549]]\n",
      "\n",
      " [[ 0.965       0.18377976]\n",
      "  [ 0.955       0.20730412]\n",
      "  [ 0.96        0.19595918]\n",
      "  [ 0.945       0.22798026]\n",
      "  [ 0.95        0.21794495]]\n",
      "\n",
      " [[ 0.921875    0.26836819]\n",
      "  [ 0.93125     0.25302853]\n",
      "  [ 0.9125      0.28256636]\n",
      "  [ 0.93125     0.25302853]\n",
      "  [ 0.928125    0.25828082]]\n",
      "\n",
      " [[ 0.92        0.2712932 ]\n",
      "  [ 0.935       0.24652586]\n",
      "  [ 0.92        0.2712932 ]\n",
      "  [ 0.9375      0.24206146]\n",
      "  [ 0.9175      0.27512497]]\n",
      "\n",
      " [[ 0.92375     0.2653977 ]\n",
      "  [ 0.93375     0.24871859]\n",
      "  [ 0.94        0.23748684]\n",
      "  [ 0.94625     0.2255237 ]\n",
      "  [ 0.9375      0.24206146]]\n",
      "\n",
      " [[ 0.904       0.29459124]\n",
      "  [ 0.935       0.24652586]\n",
      "  [ 0.935       0.24652586]\n",
      "  [ 0.938       0.24115555]\n",
      "  [ 0.931       0.25345414]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:712: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:715: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.zeros((len(S), len(classifiers), 2), dtype=np.dtype('float64'))\n",
    "for idx1, s in enumerate(S):\n",
    "    s0=s/2\n",
    "    s1=s/2\n",
    "\n",
    "    g0 = 1 * (np.random.rand( r, r, s0) > 1-p0)\n",
    "    g1 = 1 * (np.random.rand( r, r, s1) > 1-p1)\n",
    "    mbar0 = 1.0*np.sum(g0, axis=(0,1))\n",
    "    mbar1 = 1.0*np.sum(g1, axis=(0,1))\n",
    "\n",
    "    X = np.array((np.append(mbar0, mbar1), np.append(mbar0/( r**2), mbar1/( r**2 )))).T\n",
    "    y = np.append(np.zeros(s0), np.ones(s1))\n",
    "    \n",
    "    for idx2, cla in enumerate(classifiers):\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "        clf = cla.fit(X_train, y_train)\n",
    "        loo = LeaveOneOut(len(X))\n",
    "        scores = cross_validation.cross_val_score(clf, X, y, cv=loo)\n",
    "        accuracy[idx1, idx2,] = [scores.mean(), scores.std()]\n",
    "        print(\"Accuracy of %s: %0.2f (+/- %0.2f)\" % (names[idx2], scores.mean(), scores.std() * 2))\n",
    "    \n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Plot Accuracy versus N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.errorbar(S, accuracy[:,0,0], yerr = accuracy[:,0,1]/np.sqrt(S), hold=True, label=names[0])\n",
    "plt.errorbar(S, accuracy[:,1,0], yerr = accuracy[:,1,1]/np.sqrt(S), color='green', hold=True, label=names[1])\n",
    "plt.errorbar(S, accuracy[:,2,0], yerr = accuracy[:,2,1]/np.sqrt(S), color='red', hold=True, label=names[2])\n",
    "plt.errorbar(S, accuracy[:,3,0], yerr = accuracy[:,3,1]/np.sqrt(S), color='black', hold=True, label=names[3])\n",
    "plt.errorbar(S, accuracy[:,4,0], yerr = accuracy[:,4,1]/np.sqrt(S), color='brown', hold=True, label=names[4])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.xlim((0,2100))\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gender Classification of Simulated Data')\n",
    "plt.axhline(1, color='red', linestyle='--')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig('../figs/general_classification.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Apply technique to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ../data/KKI2009\n",
      "(70, 70, 42)\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataset names\n",
    "dnames = list(['../data/KKI2009'])\n",
    "print \"Dataset: \" + \", \".join(dnames)\n",
    "\n",
    "# Getting graph names\n",
    "fs = list()\n",
    "for dd in dnames:\n",
    "        fs.extend([root+'/'+file for root, dir, files in os.walk(dd) for file in files])\n",
    "fs = fs[1:]\n",
    "def loadGraphs(filenames, rois, printer=False):\n",
    "    A = np.zeros((rois, rois, len(filenames)))\n",
    "    for idx, files in enumerate(filenames):\n",
    "        if printer:\n",
    "            print \"Loading: \" + files\n",
    "        g = ig.Graph.Read_GraphML(files)\n",
    "        tempg = g.get_adjacency(attribute='weight')\n",
    "        A[:,:,idx] = np.asarray(tempg.data)\n",
    "        \n",
    "    return A\n",
    "\n",
    "# Load X\n",
    "X = loadGraphs(fs, 70)\n",
    "print X.shape\n",
    "\n",
    "# Load Y\n",
    "ys = csv.reader(open('../data/kki42_subjectinformation.csv'))\n",
    "y = [y[5] for y in ys]\n",
    "y = [1 if x=='F' else 0 for x in y[1:]]\n",
    "\n",
    "xf = 1.0*np.sum(1.0*(X>0), axis=(0,1))\n",
    "features = np.array((xf, xf/( 70**2 * 22))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Nearest Neighbors: 0.48 (+/- 1.00)\n",
      "Accuracy of Linear SVM: 0.55 (+/- 1.00)\n",
      "Accuracy of Random Forest: 0.57 (+/- 0.99)\n",
      "Accuracy of Linear Discriminant Analysis: 0.45 (+/- 1.00)\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.71 (+/- 0.90)\n"
     ]
    }
   ],
   "source": [
    "accuracy=np.zeros((len(classifiers),2))\n",
    "for idx, cla in enumerate(classifiers):\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, y, test_size=0.4, random_state=0)\n",
    "    clf = cla.fit(X_train, y_train)\n",
    "    loo = LeaveOneOut(len(features))\n",
    "    scores = cross_validation.cross_val_score(clf, features, y, cv=loo)\n",
    "    accuracy[idx,] = [scores.mean(), scores.std()]\n",
    "    print(\"Accuracy of %s: %0.2f (+/- %0.2f)\" % (names[idx], scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Reflect on result\n",
    "\n",
    "The classification accuracy on real data based on the five tested classifiers is, at best, 71%, and worst, chance. Next, I need to test my assumptions to see if they are accurate and adjust my processing/features to better represent my true scenario than the assumed conditions, if possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
